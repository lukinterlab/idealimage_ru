"""
–°–∏—Å—Ç–µ–º–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞.

–ò—â–µ—Ç –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ —á–µ—Ä–µ–∑ Google/Yandex/RSS/—Å–æ—Ü—Å–µ—Ç–∏,
–ø–∞—Ä—Å–∏—Ç –∏—Ö –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–ª—è –º–æ–¥–µ—Ä–∞—Ü–∏–∏.
"""
import logging
import re
from typing import Dict, List, Optional
from django.utils import timezone
from django.utils.html import strip_tags
from django.db import transaction

from blog.models import Category
from .models import ParsedArticle, ParsingCategory
from .universal_parser import UniversalParser

logger = logging.getLogger(__name__)


def extract_first_words(text: str, word_count: int = 200) -> str:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–µ—Ä–≤—ã–µ N —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞.
    
    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        word_count: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤
    
    Returns:
        –¢–µ–∫—Å—Ç —Å –ø–µ—Ä–≤—ã–º–∏ N —Å–ª–æ–≤–∞–º–∏
    """
    words = text.split()
    return ' '.join(words[:word_count])


def parse_popular_articles() -> Dict:
    """
    –ü–∞—Ä—Å–∏—Ç –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞.
    
    –î–ª—è –∫–∞–∂–¥–æ–π –∞–∫—Ç–∏–≤–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞:
    1. –§–æ—Ä–º–∏—Ä—É–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    2. –ò—â–µ—Ç —Å—Ç–∞—Ç—å–∏ —á–µ—Ä–µ–∑ Google/Yandex/RSS/—Å–æ—Ü—Å–µ—Ç–∏
    3. –ü–∞—Ä—Å–∏—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ (~200 —Å–ª–æ–≤)
    4. –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ ParsedArticle —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º pending
    5. –†–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º —Å–∞–π—Ç–∞
    
    Returns:
        Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞
    """
    logger.info("=" * 60)
    logger.info("üì∞ –ù–ê–ß–ê–õ–û –ü–ê–†–°–ò–ù–ì–ê –ü–û–ü–£–õ–Ø–†–ù–´–• –°–¢–ê–¢–ï–ô")
    logger.info("=" * 60)
    
    results = {
        'categories_processed': 0,
        'articles_found': 0,
        'articles_parsed': 0,
        'articles_saved': 0,
        'errors': []
    }
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º –∞–∫—Ç–∏–≤–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞
        parsing_categories = ParsingCategory.objects.filter(is_active=True)
        results['categories_processed'] = parsing_categories.count()
        
        logger.info(f"üìã –ù–∞–π–¥–µ–Ω–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –ø–∞—Ä—Å–∏–Ω–≥–∞: {results['categories_processed']}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞—Ä—Å–µ—Ä
        parser = UniversalParser()
        
        # –î–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞
        for parsing_category in parsing_categories:
            try:
                logger.info(f"üîç –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {parsing_category.name}")
                
                # –ü–æ–ª—É—á–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
                search_queries = parsing_category.search_queries or []
                if not search_queries:
                    logger.warning(f"   ‚ö†Ô∏è –ù–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {parsing_category.name}")
                    continue
                
                # –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
                sources = parsing_category.sources or []
                if not sources:
                    logger.warning(f"   ‚ö†Ô∏è –ù–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {parsing_category.name}")
                    continue
                
                # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π –≤ –¥–µ–Ω—å
                articles_per_day = parsing_category.articles_per_day or 5
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–∫–æ–ª—å–∫–æ —É–∂–µ —Å–ø–∞—Ä—à–µ–Ω–æ —Å–µ–≥–æ–¥–Ω—è
                today_start = timezone.now().replace(hour=0, minute=0, second=0, microsecond=0)
                already_parsed_today = ParsedArticle.objects.filter(
                    parsing_category=parsing_category,
                    parsed_at__gte=today_start
                ).count()
                
                if already_parsed_today >= articles_per_day:
                    logger.info(f"   ‚è≠Ô∏è –£–∂–µ —Å–ø–∞—Ä—à–µ–Ω–æ {already_parsed_today} —Å—Ç–∞—Ç–µ–π —Å–µ–≥–æ–¥–Ω—è (–ª–∏–º–∏—Ç: {articles_per_day})")
                    continue
                
                remaining = articles_per_day - already_parsed_today
                logger.info(f"   üìä –ù—É–∂–Ω–æ —Å–ø–∞—Ä—Å–∏—Ç—å –µ—â–µ {remaining} —Å—Ç–∞—Ç–µ–π")
                
                articles_found = 0
                articles_parsed = 0
                
                # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
                for query in search_queries[:3]:  # –ú–∞–∫—Å–∏–º—É–º 3 –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—é
                    if articles_parsed >= remaining:
                        break
                    
                    try:
                        logger.info(f"   üîé –ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'")
                        
                        # –ò—â–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —á–µ—Ä–µ–∑ UniversalParser
                        if 'google' in sources or 'yandex' in sources:
                            sources_list = parser.search_sources(query, limit=10)
                            results['articles_found'] += len(sources_list)
                            articles_found += len(sources_list)
                            
                            # –ü–∞—Ä—Å–∏–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
                            for source in sources_list[:remaining]:
                                if articles_parsed >= remaining:
                                    break
                                
                                try:
                                    url = source.get('url', '')
                                    if not url:
                                        continue
                                    
                                    # –ü–∞—Ä—Å–∏–º —Å—Ç–∞—Ç—å—é
                                    logger.info(f"      üì• –ü–∞—Ä—Å–∏–Ω–≥: {url[:80]}...")
                                    parsed_data = parser.parse_article(url, retries=2)
                                    
                                    if not parsed_data.get('success'):
                                        logger.warning(f"      ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–ø–∞—Ä—Å–∏—Ç—å: {url}")
                                        continue
                                    
                                    title = parsed_data.get('title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞')
                                    text = parsed_data.get('text', '')
                                    
                                    if len(text) < 100:
                                        logger.warning(f"      ‚ö†Ô∏è –¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
                                        continue
                                    
                                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–µ—Ä–≤—ã–µ ~200 —Å–ª–æ–≤
                                    content = extract_first_words(text, 200)
                                    
                                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Å–∞–π—Ç–∞
                                    site_category = parsing_category.site_category
                                    if not site_category:
                                        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
                                        try:
                                            site_category = Category.objects.filter(
                                                title__icontains=parsing_category.name
                                            ).first()
                                        except:
                                            pass
                                    
                                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å–ø–∞—Ä—à–µ–Ω–∞ –ª–∏ —É–∂–µ —ç—Ç–∞ —Å—Ç–∞—Ç—å—è
                                    existing = ParsedArticle.objects.filter(
                                        source_url=url
                                    ).first()
                                    
                                    if existing:
                                        logger.info(f"      ‚è≠Ô∏è –°—Ç–∞—Ç—å—è —É–∂–µ —Å–ø–∞—Ä—à–µ–Ω–∞ —Ä–∞–Ω–µ–µ: {title[:50]}")
                                        continue
                                    
                                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∞—Ä—à–µ–Ω–Ω—É—é —Å—Ç–∞—Ç—å—é
                                    parsed_article = ParsedArticle.objects.create(
                                        title=title[:500],
                                        content=content,
                                        source_url=url,
                                        source_name=source.get('title', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫')[:200],
                                        category=site_category,
                                        parsing_category=parsing_category,
                                        status='pending',
                                        popularity_score=source.get('popularity_score', 0)
                                    )
                                    
                                    articles_parsed += 1
                                    results['articles_parsed'] += 1
                                    results['articles_saved'] += 1
                                    
                                    logger.info(f"      ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {title[:50]}...")
                                    
                                except Exception as e:
                                    error_msg = f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç–∞—Ç—å–∏ {url}: {str(e)}"
                                    logger.error(f"      ‚ùå {error_msg}", exc_info=True)
                                    results['errors'].append(error_msg)
                        
                        # RSS –ª–µ–Ω—Ç—ã (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã)
                        if 'rss' in sources and articles_parsed < remaining:
                            # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–∞—Ä—Å–∏–Ω–≥ RSS –ª–µ–Ω—Ç
                            logger.info(f"   üì° RSS –ø–∞—Ä—Å–∏–Ω–≥ –ø–æ–∫–∞ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω")
                        
                        # –°–æ—Ü—Å–µ—Ç–∏ (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã)
                        if 'social' in sources and articles_parsed < remaining:
                            # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–∞—Ä—Å–∏–Ω–≥ —Å–æ—Ü—Å–µ—Ç–µ–π
                            logger.info(f"   üì± –ü–∞—Ä—Å–∏–Ω–≥ —Å–æ—Ü—Å–µ—Ç–µ–π –ø–æ–∫–∞ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω")
                    
                    except Exception as e:
                        error_msg = f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞ '{query}': {str(e)}"
                        logger.error(f"   ‚ùå {error_msg}", exc_info=True)
                        results['errors'].append(error_msg)
                
                logger.info(f"   ‚úÖ –ö–∞—Ç–µ–≥–æ—Ä–∏—è '{parsing_category.name}': –Ω–∞–π–¥–µ–Ω–æ {articles_found}, —Å–ø–∞—Ä—à–µ–Ω–æ {articles_parsed}")
                
            except Exception as e:
                error_msg = f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ {parsing_category.name}: {str(e)}"
                logger.error(f"‚ùå {error_msg}", exc_info=True)
                results['errors'].append(error_msg)
        
        logger.info("=" * 60)
        logger.info("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–ê–†–°–ò–ù–ì–ê:")
        logger.info(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {results['categories_processed']}")
        logger.info(f"   –°—Ç–∞—Ç–µ–π –Ω–∞–π–¥–µ–Ω–æ: {results['articles_found']}")
        logger.info(f"   –°—Ç–∞—Ç–µ–π —Å–ø–∞—Ä—à–µ–Ω–æ: {results['articles_parsed']}")
        logger.info(f"   –°—Ç–∞—Ç–µ–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {results['articles_saved']}")
        if results['errors']:
            logger.warning("   –û—à–∏–±–æ–∫: %d", len(results['errors']))
        logger.info("=" * 60)
        
    except Exception as e:
        error_msg = f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {str(e)}"
        logger.error(f"‚ùå {error_msg}", exc_info=True)
        results['errors'].append(error_msg)
    
    return results

