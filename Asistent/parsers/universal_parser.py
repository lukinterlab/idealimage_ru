"""
–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç: –≤–µ–±-—Å–∞–π—Ç—ã, YouTube, VK, Rutube, Dzen
–û–±—Ö–æ–¥ –∑–∞—â–∏—Ç—ã: User-Agent rotation, JS rendering, fallback –º–µ—Ç–æ–¥—ã
"""
import re
import logging
import requests
import warnings
from typing import List, Dict, Optional
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import time

# –§–∏–ª—å—Ç—Ä—É–µ–º warning'–∏ BeautifulSoup –æ –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö —Å–ø–∏—Å–∫–∞—Ö (–Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –≤ –∫–æ–Ω—Å–æ–ª–∏)
warnings.filterwarnings('ignore', message='.*Ignoring nested list.*')
warnings.filterwarnings('ignore', category=UserWarning, module='bs4')

logger = logging.getLogger(__name__)


class UniversalParser:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –æ–±—Ö–æ–¥–∞ –∑–∞—â–∏—Ç—ã"""
    
    # –†–æ—Ç–∞—Ü–∏—è User-Agent –¥–ª—è –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
    USER_AGENTS = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
    ]
    
    def __init__(self):
        self.session = requests.Session()
        self.current_ua_index = 0
        self._rotate_user_agent()
    
    @staticmethod
    def _normalize_url(url: str) -> Optional[str]:
        """–û—á–∏—â–∞–µ—Ç –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç URL (–æ–±—Ä–µ–∑–∞–µ—Ç –∫–∞–≤—ã—á–∫–∏, –ø—Ä–æ–±–µ–ª—ã, –¥–æ–±–∞–≤–ª—è–µ—Ç —Å—Ö–µ–º—É)."""
        if not url:
            return None
        
        cleaned = url.strip().strip('\'"').strip()
        if not cleaned:
            return None
        
        cleaned = cleaned.replace(' ', '').replace('\r', '').replace('\n', '')
        
        parsed = urlparse(cleaned)
        if not parsed.scheme:
            cleaned = f"https://{cleaned.lstrip('/')}"
            parsed = urlparse(cleaned)
        
        if not parsed.netloc:
            return None
        
        return cleaned
    
    def _rotate_user_agent(self):
        """–†–æ—Ç–∞—Ü–∏—è User-Agent"""
        self.session.headers.update({
            'User-Agent': self.USER_AGENTS[self.current_ua_index],
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        })
        self.current_ua_index = (self.current_ua_index + 1) % len(self.USER_AGENTS)
    
    def search_sources(self, query: str, limit: int = 5, only_external: bool = True) -> List[Dict]:
        """
        –ü–æ–∏—Å–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            only_external: –¢–æ–ª—å–∫–æ –≤–Ω–µ—à–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (–Ω–µ –Ω–∞—à —Å–∞–π—Ç)
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∫–ª—é—á–∞–º–∏: url, title, snippet
        """
        logger.info(f"üîç –ü–æ–∏—Å–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: query='{query}', limit={limit}")
        
        sources = []
        
        # –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å–∞–π—Ç—ã –ø–æ —Ç–µ–º–∞–º (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ)
        priority_sites = {
            '–º–æ–¥–∞': ['vogue.ru', 'elle.ru', 'bazaar.ru', 'cosmo.ru'],
            '–∫—Ä–∞—Å–æ—Ç–∞': ['cosmo.ru', 'wday.ru', 'psychologies.ru'],
            '–∑–¥–æ—Ä–æ–≤—å–µ': ['zdorovie.ru', 'medportal.ru', 'med.ru'],
            '–∫—É–ª–∏–Ω–∞—Ä–∏—è': ['eda.ru', 'gastronom.ru', 'povarenok.ru'],
        }
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–º—É
        topic_keywords = {
            '–º–æ–¥–∞': ['–º–æ–¥–∞', '–æ–¥–µ–∂–¥–∞', '—Å—Ç–∏–ª—å', '—Ç—Ä–µ–Ω–¥', 'outfit', 'fashion'],
            '–∫—Ä–∞—Å–æ—Ç–∞': ['–∫—Ä–∞—Å–æ—Ç–∞', '–º–∞–∫–∏—è–∂', '—É—Ö–æ–¥', '–∫–æ—Å–º–µ—Ç–∏–∫–∞', 'beauty'],
            '–∑–¥–æ—Ä–æ–≤—å–µ': ['–∑–¥–æ—Ä–æ–≤—å–µ', '—Ñ–∏—Ç–Ω–µ—Å', '—Å–ø–æ—Ä—Ç', 'wellness'],
            '–∫—É–ª–∏–Ω–∞—Ä–∏—è': ['—Ä–µ—Ü–µ–ø—Ç', '–≥–æ—Ç–æ–≤–∏—Ç—å', '–±–ª—é–¥–æ', '–µ–¥–∞', '–∫—É—Ö–Ω—è'],
        }
        
        detected_topic = None
        query_lower = query.lower()
        for topic, keywords in topic_keywords.items():
            if any(kw in query_lower for kw in keywords):
                detected_topic = topic
                break
        
        # –ï—Å–ª–∏ —Ç–µ–º–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ - –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Å–∞–π—Ç—ã
        if detected_topic and detected_topic in priority_sites:
            for site in priority_sites[detected_topic]:
                sources.append({
                    'url': f'https://{site}/search?q={query}',
                    'title': f'{site} - {query}',
                    'snippet': f'–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –ø–æ —Ç–µ–º–µ {detected_topic}',
                    'priority': True
                })
                if len(sources) >= limit:
                    break
        
        # –ï—Å–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ - –¥–æ–±–∞–≤–ª—è–µ–º —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        if len(sources) < limit:
            universal_sources = [
                f'https://www.google.com/search?q={query}',
                f'https://yandex.ru/search/?text={query}',
            ]
            for url in universal_sources:
                if len(sources) >= limit:
                    break
                sources.append({
                    'url': url,
                    'title': f'–ü–æ–∏—Å–∫: {query}',
                    'snippet': '–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫',
                    'priority': False
                })
        
        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(sources)}")
        return sources[:limit]
    
    def parse_feed(self, feed_url: str, limit: int = 10, extract_popularity: bool = True) -> List[Dict]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –ª–µ–Ω—Ç—ã —Å—Ç–∞—Ç–µ–π —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏
        
        Args:
            feed_url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–æ —Å–ø–∏—Å–∫–æ–º —Å—Ç–∞—Ç–µ–π
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
            extract_popularity: –ò–∑–≤–ª–µ–∫–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ (–ª–∞–π–∫–∏, –ø—Ä–æ—Å–º–æ—Ç—Ä—ã)
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å url, title, popularity_score
        """
        logger.info(f"üì∞ –ü–∞—Ä—Å–∏–Ω–≥ –ª–µ–Ω—Ç—ã: {feed_url}")
        
        try:
            response = self.session.get(feed_url, timeout=15, allow_redirects=True)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            articles = []
            
            from urllib.parse import urljoin, urlparse
            base_domain = f"{urlparse(feed_url).scheme}://{urlparse(feed_url).netloc}"
            
            # –î–ª—è VC.ru –∏—â–µ–º –±–ª–æ–∫–∏ —Å—Ç–∞—Ç–µ–π —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
            if 'vc.ru' in feed_url or 'tj.ru' in feed_url or 'dtf.ru' in feed_url:
                # –ò—â–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã —Å—Ç–∞—Ç–µ–π
                content_blocks = soup.select('.content, .content--short')
                
                for block in content_blocks:
                    try:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ —Å—Ç–∞—Ç—å—é
                        title_link = block.select_one('a.content-title, .content-title a')
                        if not title_link:
                            continue
                        
                        href = title_link.get('href')
                        if not href:
                            continue
                        
                        full_url = urljoin(base_domain, href)
                        if not self._is_article_url(full_url):
                            continue
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                        title = title_link.get_text(strip=True)
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏
                        popularity_score = 0
                        if extract_popularity:
                            # –õ–∞–π–∫–∏/—Ä–µ–∞–∫—Ü–∏–∏
                            reactions = block.select('.reaction-button__label')
                            for reaction in reactions:
                                try:
                                    count = reaction.get_text(strip=True)
                                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º "1.5K" –≤ —á–∏—Å–ª–æ
                                    if 'K' in count or '–∫' in count.lower():
                                        popularity_score += float(count.replace('K', '').replace('–∫', '').replace(',', '.')) * 1000
                                    else:
                                        popularity_score += float(count.replace(',', ''))
                                except:
                                    pass
                            
                            # –ü—Ä–æ—Å–º–æ—Ç—Ä—ã
                            views = block.select_one('.content-views-item .content-footer-button__label')
                            if views:
                                try:
                                    count = views.get_text(strip=True)
                                    if 'K' in count or '–∫' in count.lower():
                                        popularity_score += float(count.replace('K', '').replace('–∫', '').replace(',', '.')) * 1000 * 0.1  # –≤–µ—Å –º–µ–Ω—å—à–µ —á–µ–º –ª–∞–π–∫–∏
                                    else:
                                        popularity_score += float(count.replace(',', '')) * 0.1
                                except:
                                    pass
                        
                        articles.append({
                            'url': full_url,
                            'title': title,
                            'popularity_score': popularity_score
                        })
                        
                        if len(articles) >= limit:
                            break
                            
                    except Exception as e:
                        continue
            
            else:
                # –£–õ–£–ß–®–ï–ù–ù–´–ô –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è –õ–Æ–ë–´–• —Å–∞–π—Ç–æ–≤
                logger.info(f"   üîç –ò—Å–ø–æ–ª—å–∑—É—é —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è {feed_url}")
                
                # –®–ê–ì 1: –ò—â–µ–º –í–°–ï —Å—Å—ã–ª–∫–∏ —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
                link_patterns = [
                    # –°—Ç–∞—Ç—å–∏ –∏ –ø–æ—Å—Ç—ã
                    'article a[href]',
                    'article h2 a', 'article h3 a',
                    # –ó–∞–≥–æ–ª–æ–≤–∫–∏
                    'h1 a[href]', 'h2 a[href]', 'h3 a[href]',
                    # –ö–ª–∞—Å—Å—ã –ø–æ—Å—Ç–æ–≤
                    '.post a[href]', '.post-title a', '.entry-title a',
                    '.article a[href]', '.article-title a',
                    '.content a[href]', '.content-title a',
                    # –ë–ª–æ–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π
                    '.news-item a', '.blog-post a',
                    # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã
                    '[class*="post"] a[href]',
                    '[class*="article"] a[href]',
                    '[class*="entry"] a[href]',
                    '[class*="item"] a[href]',
                    # –°–ø–∏—Å–∫–∏
                    'ul li a[href]', 'ol li a[href]',
                ]
                
                seen_urls = set()  # –î–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                
                for selector in link_patterns:
                    if len(articles) >= limit:
                        break
                    
                    try:
                        links = soup.select(selector)
                        logger.info(f"      ‚Ä¢ –°–µ–ª–µ–∫—Ç–æ—Ä '{selector}': –Ω–∞–π–¥–µ–Ω–æ {len(links)} —Å—Å—ã–ª–æ–∫")
                        
                        for link in links:
                            if len(articles) >= limit:
                                break
                            
                            href = link.get('href')
                            if not href:
                                continue
                            
                            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –ø–æ–ª–Ω—ã–π URL
                            full_url = urljoin(base_domain, href)
                            
                            # –ò–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                            if full_url in seen_urls:
                                continue
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ —Å—Ç–∞—Ç—å—è
                            if not self._is_article_url(full_url):
                                continue
                            
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                            title = link.get_text(strip=True)
                            
                            # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞ (–æ—Ç—Å–µ–∫–∞–µ–º "–ß–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ" –∏ —Ç.–ø.)
                            if len(title) < 10:
                                continue
                            
                            seen_urls.add(full_url)
                            articles.append({
                                'url': full_url,
                                'title': title,
                                'popularity_score': 0
                            })
                            
                    except Exception as e:
                        logger.warning(f"      ‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º '{selector}': {e}")
                        continue
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ (–æ—Ç –±–æ–ª—å—à–µ–π –∫ –º–µ–Ω—å—à–µ–π)
            if extract_popularity:
                articles.sort(key=lambda x: x['popularity_score'], reverse=True)
            
            # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            logger.info(f"")
            logger.info(f"   üìä –ò–¢–û–ì–û –Ω–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(articles)}")
            if articles:
                logger.info(f"   ‚úÖ –°—Ç–∞—Ç—å–∏ —É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω—ã –∏–∑ {feed_url}")
                logger.info(f"   üìù –ü—Ä–∏–º–µ—Ä—ã –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤:")
                for i, art in enumerate(articles[:3]):
                    logger.info(f"      {i+1}. {art['title'][:60]}...")
                if extract_popularity and articles[0]['popularity_score'] > 0:
                    logger.info(f"   üî• –¢–æ–ø —Å—Ç–∞—Ç—å—è: score={articles[0]['popularity_score']:.0f}")
            else:
                logger.warning(f"   ‚ö†Ô∏è –ù–ò –û–î–ù–û–ô —Å—Ç–∞—Ç—å–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ!")
                logger.warning(f"   üí° –í–æ–∑–º–æ–∂–Ω–æ, —Å–∞–π—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é –∑–∞–≥—Ä—É–∑–∫—É (JS)")
                logger.warning(f"   üí° –ò–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ HTML –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –ø–æ–¥ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã")
            
            return articles[:limit]
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ª–µ–Ω—Ç—ã {feed_url}: {e}")
            return []
    
    def _is_article_url(self, url: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ URL —Å—Å—ã–ª–∫–æ–π –Ω–∞ —Å—Ç–∞—Ç—å—é
        –£–ü–†–û–©–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –±–æ–ª—å—à–µ —Å—Ç–∞—Ç–µ–π!
        """
        url = self._normalize_url(url)
        if not url:
            return False
        
        # –ò—Å–∫–ª—é—á–∞–µ–º –¢–û–õ–¨–ö–û —è–≤–Ω–æ —Å–ª—É–∂–µ–±–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        exclude_patterns = [
            '/login', '/register', '/logout',
            '/search?', '/about', '/contacts',
            'javascript:', 'mailto:',
            '/tag/', '/tags/',
            '/category/', '/categories/',
        ]
        
        url_lower = url.lower()
        for pattern in exclude_patterns:
            if pattern in url_lower:
                return False
        
        # –ï—Å–ª–∏ URL –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å # - —ç—Ç–æ —è–∫–æ—Ä—å, –Ω–µ —Å—Ç–∞—Ç—å—è
        if url.startswith('#'):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É (–æ—Ç—Å–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –≥–ª–∞–≤–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã)
        parsed = urlparse(url)
        path = parsed.path.strip('/')
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å–∞–π—Ç–∞
        if not path or path in ['index.html', 'index.php']:
            return False
        
        # –í–°–ï –û–°–¢–ê–õ–¨–ù–û–ï - –ü–†–û–ü–£–°–ö–ê–ï–ú (—à–∏—Ä–æ–∫–∏–π —Ñ–∏–ª—å—Ç—Ä!)
        return True
    
    def parse_article(self, url: str, retries: int = 3, download_images: bool = False) -> Dict:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç–∞—Ç—å–∏ —Å –ª—é–±–æ–≥–æ —Å–∞–π—Ç–∞ (—Å –æ–±—Ö–æ–¥–æ–º –∑–∞—â–∏—Ç—ã)
        
        Args:
            url: URL —Å—Ç–∞—Ç—å–∏
            retries: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫
            download_images: –°–∫–∞—á–∏–≤–∞—Ç—å –ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é False - —Ç–æ–ª—å–∫–æ –¥–ª—è —Ä–µ–∂–∏–º–∞ parse_web)
        
        Returns:
            Dict —Å –∫–ª—é—á–∞–º–∏: title, text, images, videos, author
        """
        normalized_url = self._normalize_url(url)
        if not normalized_url:
            logger.warning(f"‚ö†Ô∏è –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞: {url}")
            return self._fallback_parse(url or '')
        
        logger.info(f"üì• –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç–∞—Ç—å–∏: {normalized_url}")
        
        for attempt in range(retries):
            try:
                # –†–æ—Ç–∞—Ü–∏—è User-Agent –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–∫–∞—Ö
                if attempt > 0:
                    self._rotate_user_agent()
                    time.sleep(1)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏
                
                response = self.session.get(normalized_url, timeout=15, allow_redirects=True)
                response.raise_for_status()
                
                # –í–†–ï–ú–ï–ù–ù–û: –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML –≤ —Ñ–∞–π–ª –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                try:
                    from pathlib import Path
                    debug_file = Path(__file__).parent.parent / 'Test_Promot' / 'pars.html'
                    debug_file.parent.mkdir(parents=True, exist_ok=True)
                    with open(debug_file, 'w', encoding='utf-8') as f:
                        f.write(f"<!-- URL: {normalized_url} -->\n")
                        f.write(f"<!-- Content-Type: {response.headers.get('Content-Type', 'unknown')} -->\n")
                        f.write(f"<!-- Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')} -->\n\n")
                        f.write(response.text)
                    logger.debug(f"üíæ HTML —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {debug_file}")
                except Exception as save_error:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å HTML –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏: {save_error}")
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                content_type = response.headers.get('Content-Type', '')
                
                if 'text/html' in content_type:
                    return self._parse_html(response.text, normalized_url, download_images=download_images)
                elif 'application/json' in content_type:
                    return self._parse_json(response.json())
                else:
                    logger.warning(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π Content-Type: {content_type}")
                    return self._parse_html(response.text, normalized_url, download_images=download_images)  # –ü–æ–ø—ã—Ç–∫–∞ –∫–∞–∫ HTML
                
            except requests.exceptions.RequestException as e:
                logger.warning(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{retries} –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
                if attempt == retries - 1:
                    # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback
                    return self._fallback_parse(normalized_url)
        
        return {}
    
    def _parse_html(self, html: str, url: str, download_images: bool = False) -> Dict:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ HTML –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        
        Args:
            html: HTML —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            download_images: –°–∫–∞—á–∏–≤–∞—Ç—å –ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (—Ç–æ–ª—å–∫–æ –¥–ª—è —Ä–µ–∂–∏–º–∞ parse_web)
        """
        soup = BeautifulSoup(html, 'html.parser')
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è horo.mail.ru
        if 'horo.mail.ru' in url:
            return self._parse_horo_mail_ru(soup, url, download_images=download_images)
        
        # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã, —Å—Ç–∏–ª–∏, –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
        for element in soup(['script', 'style', 'meta', 'link', 'noscript']):
            element.decompose()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
        title = None
        if soup.find('h1'):
            title = soup.find('h1').get_text(strip=True)
        elif soup.find('title'):
            title = soup.find('title').get_text(strip=True)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç
        # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å—Ç–∞—Ç—å–∏
        article_containers = soup.find_all(['article', 'main', 
                                           soup.find(class_=re.compile(r'(article|content|post|entry)')),
                                           soup.find(id=re.compile(r'(article|content|post|entry)'))])
        
        text_parts = []
        if article_containers:
            for container in article_containers[:1]:  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π
                for p in container.find_all(['p', 'h2', 'h3', 'li']):
                    text = p.get_text(strip=True)
                    if len(text) > 20:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∞–±–∑–∞—Ü–∞
                        text_parts.append(text)
        else:
            # Fallback: —Å–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
            for p in soup.find_all('p'):
                text = p.get_text(strip=True)
                if len(text) > 20:
                    text_parts.append(text)
        
        text = '\n\n'.join(text_parts)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        images = []
        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src') or img.get('data-bg')
            if src:
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ URL –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ
                if src.startswith('//'):
                    src = 'https:' + src
                elif src.startswith('/'):
                    parsed = urlparse(url)
                    src = f"{parsed.scheme}://{parsed.netloc}{src}"
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º placeholder –∏ –º–µ–ª–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                if 'placeholder' not in src.lower() and 'empty.png' not in src.lower():
                    images.append(src)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–∏–¥–µ–æ
        videos = []
        for video in soup.find_all(['video', 'iframe']):
            src = video.get('src')
            if src:
                videos.append(src)
        
        # –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¢–û–õ–¨–ö–û –µ—Å–ª–∏ —è–≤–Ω–æ –∑–∞–ø—Ä–æ—à–µ–Ω–æ (—Ä–µ–∂–∏–º parse_web)
        downloaded_images = []
        if download_images:
            downloaded_images = self._download_images_from_urls(images[:10], url)
            logger.info(f"‚úÖ –°–ø–∞—Ä—à–µ–Ω–æ: –∑–∞–≥–æ–ª–æ–≤–æ–∫={bool(title)}, —Ç–µ–∫—Å—Ç={len(text)} —Å–∏–º–≤–æ–ª–æ–≤, "
                       f"–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π={len(images)}, —Å–∫–∞—á–∞–Ω–æ={len(downloaded_images)}, –≤–∏–¥–µ–æ={len(videos)}")
        else:
            logger.info(f"‚úÖ –°–ø–∞—Ä—à–µ–Ω–æ: –∑–∞–≥–æ–ª–æ–≤–æ–∫={bool(title)}, —Ç–µ–∫—Å—Ç={len(text)} —Å–∏–º–≤–æ–ª–æ–≤, "
                       f"–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π={len(images)} (—Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –æ—Ç–∫–ª—é—á–µ–Ω–æ), –≤–∏–¥–µ–æ={len(videos)}")
        
        return {
            'title': title or '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞',
            'text': text,
            'images': images[:10],  # URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            'downloaded_images': downloaded_images,  # –ü–£–¢–ò –∫ —Å–∫–∞—á–∞–Ω–Ω—ã–º —Ñ–∞–π–ª–∞–º! (–ø—É—Å—Ç–æ –µ—Å–ª–∏ download_images=False)
            'videos': videos[:3],    # –ú–∞–∫—Å–∏–º—É–º 3 –≤–∏–¥–µ–æ
            'url': url,
            'success': bool(text and len(text) > 50),  # –£—Å–ø–µ—Ö –µ—Å–ª–∏ –µ—Å—Ç—å —Ç–µ–∫—Å—Ç > 50 —Å–∏–º–≤–æ–ª–æ–≤
        }
    
    def _parse_horo_mail_ru(self, soup: BeautifulSoup, url: str, download_images: bool = False) -> Dict:
        """
        –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è horo.mail.ru
        –ò—â–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –≥–æ—Ä–æ—Å–∫–æ–ø–∞ –≤ HTML —Å—Ç—Ä—É–∫—Ç—É—Ä–µ —Å–∞–π—Ç–∞
        
        Args:
            soup: BeautifulSoup –æ–±—ä–µ–∫—Ç
            url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            download_images: –°–∫–∞—á–∏–≤–∞—Ç—å –ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ (–∫–∞–∫ _parse_html)
        """
        # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã, —Å—Ç–∏–ª–∏, –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
        for element in soup(['script', 'style', 'meta', 'link', 'noscript']):
            element.decompose()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
        title = None
        h1_element = soup.find('h1', class_=re.compile(r'heading'))
        if h1_element:
            title = h1_element.get_text(strip=True)
        elif soup.find('h1'):
            title = soup.find('h1').get_text(strip=True)
        elif soup.find('title'):
            title = soup.find('title').get_text(strip=True)
        
        # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –±–ª–æ–∫ –≥–æ—Ä–æ—Å–∫–æ–ø–∞
        text_parts = []
        
        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞: –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –ø—Ä–∏ –≤—Å—Ç—Ä–µ—á–µ —ç—Ç–∏—Ö —Å–ª–æ–≤
        stop_words = ['–§–∏–Ω–∞–Ω—Å—ã', '–ó–¥–æ—Ä–æ–≤—å–µ', '–õ—é–±–æ–≤—å']
        
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è horo.mail.ru
        selectors = [
            # –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –≥–æ—Ä–æ—Å–∫–æ–ø–∞
            'div[data-qa="HoroscopeText"] p',
            'div[data-qa*="Horoscope"] p',
            'article p',
            '.article p',
            '[class*="article"] p',
            '[class*="content"] p',
            '[class*="horoscope"] p',
            '[data-qa*="content"] p',
            # –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã —Å —Ç–µ–∫—Å—Ç–æ–º
            'main p',
            '.main-content p',
            '[role="main"] p',
        ]
        
        found_content = False
        for selector in selectors:
            elements = soup.select(selector)
            if elements:
                logger.debug(f"   –ù–∞–π–¥–µ–Ω–æ {len(elements)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä—É '{selector}'")
                for elem in elements:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–∞–º —ç–ª–µ–º–µ–Ω—Ç —Å—Å—ã–ª–∫–æ–π/–∫–Ω–æ–ø–∫–æ–π —Å–æ —Å—Ç–æ–ø-—Å–ª–æ–≤–æ–º
                    if elem.name in ['a', 'button']:
                        elem_text = elem.get_text(strip=True)
                        elem_text_lower = elem_text.lower()
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∏–ª–∏ –∫–æ—Ä–æ—Ç–∫–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                        if any(stop_word.lower() == elem_text_lower or 
                               (stop_word.lower() in elem_text_lower and len(elem_text) <= 15) 
                               for stop_word in stop_words):
                            logger.info(f"   ‚õî –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: –Ω–∞–π–¥–µ–Ω–æ —Å—Ç–æ–ø-—Å–ª–æ–≤–æ –≤ —Å—Å—ã–ª–∫–µ '{elem_text}'")
                            found_content = True
                            break
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤–Ω—É—Ç—Ä–∏ —ç–ª–µ–º–µ–Ω—Ç–∞ —Å—Å—ã–ª–∫–∏/–∫–Ω–æ–ø–∫–∏ —Å–æ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞–º–∏
                    links_buttons = elem.find_all(['a', 'button'])
                    has_stop_word_in_link = False
                    
                    for link in links_buttons:
                        link_text = link.get_text(strip=True)
                        link_text_lower = link_text.lower()
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ (–§–∏–Ω–∞–Ω—Å—ã, –ó–¥–æ—Ä–æ–≤—å–µ, –õ—é–±–æ–≤—å)
                        if any(stop_word.lower() == link_text_lower or 
                               (stop_word.lower() in link_text_lower and len(link_text) <= 15) 
                               for stop_word in stop_words):
                            logger.info(f"   ‚õî –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: –Ω–∞–π–¥–µ–Ω–æ —Å—Ç–æ–ø-—Å–ª–æ–≤–æ –≤ —Å—Å—ã–ª–∫–µ '{link_text}'")
                            found_content = True
                            has_stop_word_in_link = True
                            break
                    
                    if has_stop_word_in_link:
                        break  # –ü—Ä–µ–∫—Ä–∞—â–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥
                    
                    text = elem.get_text(strip=True)
                    if len(text) > 30:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –¥–ª—è –≥–æ—Ä–æ—Å–∫–æ–ø–∞
                        text_parts.append(text)
                
                if found_content or text_parts:
                    break  # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç –∏–ª–∏ —Å—Ç–æ–ø-—Å–ª–æ–≤–æ, –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º –ø–æ–∏—Å–∫
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã, –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
        if not text_parts and not found_content:
            logger.debug("   –ü—Ä–æ–±—É–µ–º fallback: –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã")
            for p in soup.find_all('p'):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤–Ω—É—Ç—Ä–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ —Å—Å—ã–ª–∫–∏/–∫–Ω–æ–ø–∫–∏ —Å–æ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞–º–∏
                links_buttons = p.find_all(['a', 'button'])
                has_stop_word_in_link = False
                
                for link in links_buttons:
                    link_text = link.get_text(strip=True)
                    link_text_lower = link_text.lower()
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ (–§–∏–Ω–∞–Ω—Å—ã, –ó–¥–æ—Ä–æ–≤—å–µ, –õ—é–±–æ–≤—å)
                    if any(stop_word.lower() == link_text_lower or 
                           (stop_word.lower() in link_text_lower and len(link_text) <= 15) 
                           for stop_word in stop_words):
                        logger.info(f"   ‚õî –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: –Ω–∞–π–¥–µ–Ω–æ —Å—Ç–æ–ø-—Å–ª–æ–≤–æ –≤ —Å—Å—ã–ª–∫–µ '{link_text}'")
                        has_stop_word_in_link = True
                        break
                
                if has_stop_word_in_link:
                    break  # –ü—Ä–µ–∫—Ä–∞—â–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥
                
                text = p.get_text(strip=True)
                # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã –∏ —Å–ª—É–∂–µ–±–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                if len(text) > 30 and 'cookie' not in text.lower() and 'javascript' not in text.lower():
                    text_parts.append(text)
        
        text = '\n\n'.join(text_parts)
        
        # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –≤–∏–¥–µ–æ –Ω–µ –∏–∑–≤–ª–µ–∫–∞–µ–º - –æ–Ω–∏ –Ω–µ –Ω—É–∂–Ω—ã
        images = []
        downloaded_images = []
        videos = []
        
        logger.info(f"‚úÖ –°–ø–∞—Ä—à–µ–Ω–æ horo.mail.ru: –∑–∞–≥–æ–ª–æ–≤–æ–∫={bool(title)}, —Ç–µ–∫—Å—Ç={len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        return {
            'title': title or '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞',
            'text': text,
            'images': images[:10],
            'downloaded_images': downloaded_images,
            'videos': videos[:3],
            'url': url,
            'success': bool(text and len(text) > 50)
        }
    
    def _download_images_from_urls(self, image_urls: List[str], source_url: str) -> List[str]:
        """
        –°–∫–∞—á–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –°–†–ê–ó–£ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ
        
        Args:
            image_urls: –°–ø–∏—Å–æ–∫ URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            source_url: URL –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (–¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è)
        
        Returns:
            –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Å–∫–∞—á–∞–Ω–Ω—ã–º —Ñ–∞–π–ª–∞–º (–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç MEDIA_ROOT)
        """
        if not image_urls:
            return []
        
        logger.info(f"      üíæ –°–∫–∞—á–∏–≤–∞—é {len(image_urls)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
        
        import os
        import uuid
        from django.conf import settings
        from django.core.files.base import ContentFile
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        parsed_images_dir = os.path.join(settings.MEDIA_ROOT, 'parsed_images')
        os.makedirs(parsed_images_dir, exist_ok=True)
        
        downloaded_paths = []
        
        for idx, img_url in enumerate(image_urls[:5]):  # –°–∫–∞—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º—É–º 5 –ª—É—á—à–∏—Ö
            try:
                logger.info(f"         üì• [{idx+1}/5] {img_url[:60]}...")
                
                # –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞–ø—Ä—è–º—É—é —á–µ—Ä–µ–∑ requests
                response = self.session.get(img_url, timeout=10, stream=True)
                response.raise_for_status()
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                content_type = response.headers.get('Content-Type', '')
                if not content_type.startswith('image/'):
                    logger.info(f"            ‚è≠Ô∏è –ù–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (Content-Type: {content_type})")
                    continue
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
                ext = img_url.split('.')[-1].split('?')[0][:4]  # –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∏–∑ URL
                if ext not in ['jpg', 'jpeg', 'png', 'webp', 'gif']:
                    ext = 'jpg'  # –î–µ—Ñ–æ–ª—Ç–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
                
                filename = f"parsed_{uuid.uuid4().hex[:12]}.{ext}"
                file_path = os.path.join(parsed_images_dir, filename)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
                with open(file_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ (–º–∏–Ω–∏–º—É–º 10KB)
                if os.path.getsize(file_path) < 10240:
                    logger.info(f"            ‚è≠Ô∏è –°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π —Ñ–∞–π–ª (< 10KB)")
                    os.remove(file_path)
                    continue
                
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –û–¢–ù–û–°–ò–¢–ï–õ–¨–ù–´–ô –ø—É—Ç—å –æ—Ç MEDIA_ROOT
                relative_path = f"parsed_images/{filename}"
                downloaded_paths.append(relative_path)
                
                logger.info(f"            ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {relative_path}")
                    
            except Exception as e:
                logger.warning(f"            ‚ö†Ô∏è –û—à–∏–±–∫–∞: {type(e).__name__}: {str(e)[:50]}")
                continue
        
        logger.info(f"      ‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–æ: {len(downloaded_paths)} –∏–∑ {len(image_urls)}")
        return downloaded_paths
    
    def _parse_json(self, data: Dict) -> Dict:
        """–ü–∞—Ä—Å–∏–Ω–≥ JSON –¥–∞–Ω–Ω—ã—Ö (–¥–ª—è API)"""
        return {
            'title': data.get('title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞'),
            'text': data.get('content', data.get('description', '')),
            'images': data.get('images', []),
            'videos': data.get('videos', []),
            'success': True
        }
    
    def _fallback_parse(self, url: str) -> Dict:
        """Fallback –º–µ—Ç–æ–¥ –∫–æ–≥–¥–∞ –æ—Å–Ω–æ–≤–Ω–æ–π –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ —É–¥–∞–ª—Å—è"""
        logger.warning(f"‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É—é fallback –¥–ª—è {url}")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        return {
            'title': '–°—Ç–∞—Ç—å—è (–ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ —É–¥–∞–ª—Å—è)',
            'text': f'–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∫–æ–Ω—Ç–µ–Ω—Ç —Å {url}. '
                   '–ò—Å—Ç–æ—á–Ω–∏–∫ –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∑–∞—â–∏—Ç—É –æ—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞. '
                   '–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–∫–∞–∑–∞—Ç—å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫.',
            'images': [],
            'videos': [],
            'url': url,
            'success': False
        }
