"""
–í–µ–±-–ø–∞—Ä—Å–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º –ë–ï–ó API
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç: Bing, Yandex, Yahoo
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–µ–±-—Å–∫—Ä–∞–ø–∏–Ω–≥ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
"""
import requests
from bs4 import BeautifulSoup
import logging
import re
from typing import List, Optional
from urllib.parse import urljoin, quote_plus
import time
import random

logger = logging.getLogger(__name__)

"""–ü–∞—Ä—Å–µ—Ä Bing Image Search"""
class BingImageParser:
    """–ü–∞—Ä—Å–µ—Ä Bing Image Search"""
    
    def __init__(self):
        self.base_url = "https://www.bing.com/images/search"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
            'Referer': 'https://www.bing.com/'
        }
    
    def search(self, query: str, limit: int = 10) -> List[str]:
        """
        –ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ Bing
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            limit: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
        Returns:
            List URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        """
        logger.info(f"üîç Bing Images: –ø–æ–∏—Å–∫ '{query}'")
        
        try:
            params = {
                'q': query,
                'form': 'HDRSC2',
                'first': 1,
                'tsc': 'ImageHoverTitle'
            }
            
            response = requests.get(
                self.base_url,
                params=params,
                headers=self.headers,
                timeout=10
            )
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            image_urls = []
            
            # Bing –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞–∑–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã, –ø—Ä–æ–±—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
            
            # –í–∞—Ä–∏–∞–Ω—Ç 1: –∏—â–µ–º —Ç–µ–≥–∏ img —Å src
            for img in soup.find_all('img', limit=limit * 2):
                src = img.get('src') or img.get('data-src')
                if src and self._is_valid_image_url(src):
                    if src not in image_urls:
                        image_urls.append(src)
                        if len(image_urls) >= limit:
                            break
            
            # –í–∞—Ä–∏–∞–Ω—Ç 2: –∏—â–µ–º –≤ data-–∞—Ç—Ä–∏–±—É—Ç–∞—Ö (Bing —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç)
            if len(image_urls) < limit:
                for element in soup.find_all(attrs={'data-m': True}):
                    data_m = element.get('data-m', '')
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º URL –∏–∑ JSON-—Å—Ç—Ä–æ–∫–∏
                    urls = re.findall(r'"murl":"(https?://[^"]+)"', data_m)
                    for url in urls:
                        if url not in image_urls and self._is_valid_image_url(url):
                            image_urls.append(url)
                            if len(image_urls) >= limit:
                                break
                    if len(image_urls) >= limit:
                        break
            
            # –í–∞—Ä–∏–∞–Ω—Ç 3: –∏—â–µ–º –≤ href —É —Å—Å—ã–ª–æ–∫ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            if len(image_urls) < limit:
                for a_tag in soup.find_all('a', attrs={'m': True}, limit=limit * 2):
                    m_data = a_tag.get('m', '')
                    urls = re.findall(r'"murl":"(https?://[^"]+)"', m_data)
                    for url in urls:
                        if url not in image_urls and self._is_valid_image_url(url):
                            image_urls.append(url)
                            if len(image_urls) >= limit:
                                break
            
            logger.info(f"‚úÖ Bing: –Ω–∞–π–¥–µ–Ω–æ {len(image_urls)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
            return image_urls[:limit]
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Bing: {e}")
            return []
    
    def _is_valid_image_url(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        if not url or not url.startswith('http'):
            return False
        
        # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ URL Bing
        exclude_patterns = [
            'th?id=',
            'bing.com/th',
            'favicon',
            'logo',
            'pixel',
            'tracking',
            '1x1'
        ]
        
        url_lower = url.lower()
        if any(pattern in url_lower for pattern in exclude_patterns):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
        valid_extensions = ['.jpg', '.jpeg', '.png', '.webp', '.gif']
        if any(url_lower.endswith(ext) for ext in valid_extensions):
            return True
        
        # –ï—Å–ª–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –Ω–µ—Ç, –Ω–æ URL —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        if any(ext in url_lower for ext in valid_extensions):
            return True
        
        return False


"""–ü–∞—Ä—Å–µ—Ä Yandex Images"""
class YandexImageParser:
    """–ü–∞—Ä—Å–µ—Ä Yandex Images"""
    
    def __init__(self):
        self.base_url = "https://yandex.ru/images/search"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7'
        }
    
    def search(self, query: str, limit: int = 10) -> List[str]:
        """
        –ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ Yandex
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            limit: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
        Returns:
            List URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        """
        logger.info(f"üîç Yandex Images: –ø–æ–∏—Å–∫ '{query}'")
        
        try:
            params = {
                'text': query,
                'nomisspell': 1,
                'noreask': 1
            }
            
            response = requests.get(
                self.base_url,
                params=params,
                headers=self.headers,
                timeout=10
            )
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            image_urls = []
            
            # Yandex —Ö—Ä–∞–Ω–∏—Ç –¥–∞–Ω–Ω—ã–µ –≤ JSON –≤–Ω—É—Ç—Ä–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            # –ò—â–µ–º script —Ç–µ–≥–∏ —Å –¥–∞–Ω–Ω—ã–º–∏
            for script in soup.find_all('script'):
                script_content = script.string
                if script_content and 'serp-item' in script_content:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ JSON
                    urls = re.findall(r'"url":"(https?://[^"]+\.(?:jpg|jpeg|png|webp))', script_content)
                    for url in urls:
                        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º escaped —Å–∏–º–≤–æ–ª—ã
                        url = url.replace('\\/', '/')
                        if url not in image_urls and self._is_valid_image_url(url):
                            image_urls.append(url)
                            if len(image_urls) >= limit:
                                break
                
                if len(image_urls) >= limit:
                    break
            
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥: —á–µ—Ä–µ–∑ img —Ç–µ–≥–∏
            if len(image_urls) < limit:
                for img in soup.find_all('img', class_=re.compile('.*serp-item.*|.*thumb.*'), limit=limit * 2):
                    src = img.get('src') or img.get('data-src') or img.get('data-bem')
                    if src and self._is_valid_image_url(src):
                        if src not in image_urls:
                            image_urls.append(src)
                            if len(image_urls) >= limit:
                                break
            
            logger.info(f"‚úÖ Yandex: –Ω–∞–π–¥–µ–Ω–æ {len(image_urls)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
            return image_urls[:limit]
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Yandex: {e}")
            return []
    
    def _is_valid_image_url(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        if not url or not url.startswith('http'):
            return False
        
        # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ URL
        exclude_patterns = [
            'favicon',
            'logo.svg',
            'pixel',
            'avatar',
            '1x1'
        ]
        
        url_lower = url.lower()
        if any(pattern in url_lower for pattern in exclude_patterns):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
        valid_extensions = ['.jpg', '.jpeg', '.png', '.webp']
        return any(ext in url_lower for ext in valid_extensions)


"""–ü–∞—Ä—Å–µ—Ä Yahoo Image Search"""
class YahooImageParser:
    """–ü–∞—Ä—Å–µ—Ä Yahoo Image Search"""
    
    def __init__(self):
        self.base_url = "https://images.search.yahoo.com/search/images"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9'
        }
    
    def search(self, query: str, limit: int = 10) -> List[str]:
        """
        –ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ Yahoo
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            limit: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
        Returns:
            List URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        """
        logger.info(f"üîç Yahoo Images: –ø–æ–∏—Å–∫ '{query}'")
        
        try:
            params = {
                'p': query,
                'fr': 'yfp-t',
                'fr2': 'p:s,v:i'
            }
            
            response = requests.get(
                self.base_url,
                params=params,
                headers=self.headers,
                timeout=10
            )
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            image_urls = []
            
            # Yahoo –∏—Å–ø–æ–ª—å–∑—É–µ—Ç data-* –∞—Ç—Ä–∏–±—É—Ç—ã
            for img in soup.find_all('img', limit=limit * 2):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã
                src = (img.get('src') or 
                       img.get('data-src') or 
                       img.get('data-url'))
                
                if src and self._is_valid_image_url(src):
                    if src not in image_urls:
                        image_urls.append(src)
                        if len(image_urls) >= limit:
                            break
            
            # –ò—â–µ–º –≤ —Å—Å—ã–ª–∫–∞—Ö –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            if len(image_urls) < limit:
                for a_tag in soup.find_all('a', href=True, limit=limit * 2):
                    href = a_tag.get('href', '')
                    # Yahoo –∏–Ω–æ–≥–¥–∞ –æ–±–æ—Ä–∞—á–∏–≤–∞–µ—Ç URL
                    if 'imgurl=' in href:
                        match = re.search(r'imgurl=([^&]+)', href)
                        if match:
                            url = match.group(1)
                            if url not in image_urls and self._is_valid_image_url(url):
                                image_urls.append(url)
                                if len(image_urls) >= limit:
                                    break
            
            logger.info(f"‚úÖ Yahoo: –Ω–∞–π–¥–µ–Ω–æ {len(image_urls)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
            return image_urls[:limit]
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Yahoo: {e}")
            return []
    
    def _is_valid_image_url(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        if not url or not url.startswith('http'):
            return False
        
        exclude_patterns = ['favicon', 'logo', 'pixel', '1x1', 'tracking']
        url_lower = url.lower()
        
        if any(pattern in url_lower for pattern in exclude_patterns):
            return False
        
        valid_extensions = ['.jpg', '.jpeg', '.png', '.webp', '.gif']
        return any(ext in url_lower for ext in valid_extensions)


"""–ê–≥—Ä–µ–≥–∞—Ç–æ—Ä –≤–µ–±-–ø–æ–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
class WebImageSearchAggregator:
    """–ê–≥—Ä–µ–≥–∞—Ç–æ—Ä –≤–µ–±-–ø–æ–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
    
    def __init__(self):
        self.bing = BingImageParser()
        self.yandex = YandexImageParser()
        self.yahoo = YahooImageParser()
    
    def search_all(self, query: str, limit: int = 10, 
                   engines: List[str] = None) -> List[str]:
        """
        –ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤–æ –≤—Å–µ—Ö –ø–æ–∏—Å–∫–æ–≤–∏–∫–∞—Ö
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            limit: –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            engines: –°–ø–∏—Å–æ–∫ –¥–≤–∏–∂–∫–æ–≤ ['bing', 'yandex', 'yahoo'] –∏–ª–∏ None –¥–ª—è –≤—Å–µ—Ö
        
        Returns:
            List URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        """
        if engines is None:
            engines = ['bing', 'yandex', 'yahoo']
        
        all_images = []
        per_engine = (limit // len(engines)) + 1
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: Bing ‚Üí Yandex ‚Üí Yahoo
        for engine in engines:
            try:
                if engine == 'bing' and len(all_images) < limit:
                    images = self.bing.search(query, limit=per_engine)
                    all_images.extend([img for img in images if img not in all_images])
                    time.sleep(random.uniform(0.5, 1.5))  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                
                elif engine == 'yandex' and len(all_images) < limit:
                    images = self.yandex.search(query, limit=per_engine)
                    all_images.extend([img for img in images if img not in all_images])
                    time.sleep(random.uniform(0.5, 1.5))
                
                elif engine == 'yahoo' and len(all_images) < limit:
                    images = self.yahoo.search(query, limit=per_engine)
                    all_images.extend([img for img in images if img not in all_images])
                    time.sleep(random.uniform(0.5, 1.5))
            
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ {engine}: {e}")
                continue
        
        logger.info(f"‚úÖ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(all_images)}")
        return all_images[:limit]
    
    def search_best_quality(self, query: str, limit: int = 5) -> Optional[str]:
        """
        –ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            limit: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        
        Returns:
            URL –ª—É—á—à–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–ª–∏ None
        """
        images = self.search_all(query, limit=limit, engines=['bing', 'yandex'])
        
        if not images:
            return None
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤–æ–µ (–æ–±—ã—á–Ω–æ –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞)
        return images[0]


# –£–¥–æ–±–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
"""–ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –≤–µ–±"""
def search_web_images(query: str, limit: int = 10) -> List[str]:
    """
    –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –≤–µ–±
    
    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        limit: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    
    Returns:
        List URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    """
    aggregator = WebImageSearchAggregator()
    return aggregator.search_all(query, limit=limit)


"""–ü–æ–ª—É—á–∏—Ç—å –ª—É—á—à–µ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
def get_best_web_image(query: str) -> Optional[str]:
    """
    –ü–æ–ª—É—á–∏—Ç—å –ª—É—á—à–µ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ –∑–∞–ø—Ä–æ—Å—É
    
    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
    
    Returns:
        URL –ª—É—á—à–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    """
    aggregator = WebImageSearchAggregator()
    return aggregator.search_best_quality(query, limit=5)

