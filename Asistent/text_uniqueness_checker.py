"""
–ü—Ä–æ–≤–µ—Ä–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç–µ–π
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞ –¥—É–±–ª–∏ –≤ –±–∞–∑–µ –∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ
"""
import logging
from typing import Tuple, Dict
from django.utils.html import strip_tags
import re
from difflib import SequenceMatcher

logger = logging.getLogger(__name__)


class TextUniquenessChecker:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞"""
    
    # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
    MIN_UNIQUENESS = 70
    
    # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ (–≤ —Å–∏–º–≤–æ–ª–∞—Ö)
    MIN_TEXT_LENGTH = 200
    
    def __init__(self):
        pass
    
    def check_uniqueness(self, post) -> Tuple[bool, float, str]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏
        
        Args:
            post: –û–±—ä–µ–∫—Ç —Å—Ç–∞—Ç—å–∏ (Post model)
        
        Returns:
            Tuple (is_unique, uniqueness_percent, message)
        """
        logger.info(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞: {post.title}")
        
        # –ü–æ–ª—É—á–∞–µ–º —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç
        clean_text = self._clean_text(post.content)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É
        if len(clean_text) < self.MIN_TEXT_LENGTH:
            return True, 100.0, "–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏"
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 1: –î—É–±–ª–∏ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
        db_uniqueness, db_message = self._check_database_duplicates(post, clean_text)
        
        if db_uniqueness < self.MIN_UNIQUENESS:
            return False, db_uniqueness, db_message
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 2: –ò–Ω—Ç–µ—Ä–Ω–µ—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –µ—Å–ª–∏ –µ—Å—Ç—å API)
        # web_uniqueness = self._check_web_duplicates(clean_text)
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        final_uniqueness = db_uniqueness
        
        if final_uniqueness >= self.MIN_UNIQUENESS:
            return True, final_uniqueness, f"–¢–µ–∫—Å—Ç —É–Ω–∏–∫–∞–ª–µ–Ω ({final_uniqueness:.1f}%)"
        else:
            return False, final_uniqueness, f"–¢–µ–∫—Å—Ç –Ω–µ —É–Ω–∏–∫–∞–ª–µ–Ω ({final_uniqueness:.1f}%)"
    
    def _clean_text(self, content: str) -> str:
        """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç HTML –∏ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        # –£–¥–∞–ª—è–µ–º HTML —Ç–µ–≥–∏
        text = strip_tags(content)
        
        # –£–¥–∞–ª—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text)
        
        # –£–¥–∞–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        text = text.strip()
        
        return text
    
    def _check_database_duplicates(self, post, clean_text: str) -> Tuple[float, str]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞ –¥—É–±–ª–∏ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
        
        Returns:
            Tuple (uniqueness_percent, message)
        """
        try:
            from blog.models import Post
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –∫—Ä–æ–º–µ —Ç–µ–∫—É—â–µ–π
            other_posts = Post.objects.exclude(pk=post.pk if post.pk else None)
            other_posts = other_posts.filter(status='published')[:200]  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 200
            
            max_similarity = 0.0
            most_similar_post = None
            
            for other_post in other_posts:
                # –ü–æ–ª—É—á–∞–µ–º —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç –¥—Ä—É–≥–æ–π —Å—Ç–∞—Ç—å–∏
                other_text = self._clean_text(other_post.content)
                
                # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ
                similarity = self._calculate_similarity(clean_text, other_text)
                
                if similarity > max_similarity:
                    max_similarity = similarity
                    most_similar_post = other_post
                
                # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–æ 100% —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ - —Å—Ä–∞–∑—É –≤–æ–∑–≤—Ä–∞—â–∞–µ–º
                if similarity >= 0.99:
                    logger.warning(f"   ‚ùå –î–£–ë–õ–ò–ö–ê–¢! –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ {similarity*100:.1f}% —Å–æ —Å—Ç–∞—Ç—å–µ–π #{other_post.id}")
                    return 0.0, f"–î—É–±–ª–∏–∫–∞—Ç —Å—Ç–∞—Ç—å–∏ #{other_post.id} '{other_post.title}'"
            
            # –í—ã—á–∏—Å–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å (–∏–Ω–≤–µ—Ä—Å–∏—è —Å—Ö–æ–¥—Å—Ç–≤–∞)
            uniqueness = (1 - max_similarity) * 100
            
            if most_similar_post and max_similarity > 0.3:
                message = f"–ù–∞–∏–±–æ–ª—å—à–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ {max_similarity*100:.1f}% —Å–æ —Å—Ç–∞—Ç—å–µ–π #{most_similar_post.id}"
                logger.info(f"   üìä {message}")
            else:
                message = "–°–æ–≤–ø–∞–¥–µ–Ω–∏–π –≤ –±–∞–∑–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
                logger.info(f"   ‚úì {message}")
            
            return uniqueness, message
            
        except Exception as e:
            logger.error(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {e}")
            # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ —Å—á–∏—Ç–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–º
            return 100.0, f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏: {str(e)}"
    
    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ö–æ–¥—Å—Ç–≤–æ –¥–≤—É—Ö —Ç–µ–∫—Å—Ç–æ–≤
        
        Returns:
            float –æ—Ç 0.0 (—Ä–∞–∑–Ω—ã–µ) –¥–æ 1.0 (–∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ)
        """
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text1_lower = text1.lower()
        text2_lower = text2.lower()
        
        # –ú–µ—Ç–æ–¥ 1: –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –ø–µ—Ä–≤—ã–º —Å–∏–º–≤–æ–ª–∞–º
        if len(text1_lower) > 100 and len(text2_lower) > 100:
            first_chars_similarity = SequenceMatcher(
                None, 
                text1_lower[:100], 
                text2_lower[:100]
            ).ratio()
            
            # –ï—Å–ª–∏ –ø–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤ —Å–∏–ª—å–Ω–æ –æ—Ç–ª–∏—á–∞—é—Ç—Å—è - —Ç–µ–∫—Å—Ç—ã —Ä–∞–∑–Ω—ã–µ
            if first_chars_similarity < 0.3:
                return first_chars_similarity
        
        # –ú–µ—Ç–æ–¥ 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ —à–∏–Ω–≥–ª–∞–º (n-–≥—Ä–∞–º–º—ã —Å–ª–æ–≤)
        similarity_shingles = self._shingle_similarity(text1_lower, text2_lower)
        
        # –ú–µ—Ç–æ–¥ 3: –û–±—â–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –≤—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–∞ (–¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤)
        if len(text1) < 1000 and len(text2) < 1000:
            similarity_full = SequenceMatcher(None, text1_lower, text2_lower).ratio()
            # –ë–µ—Ä–µ–º –º–∞–∫—Å–∏–º—É–º –∏–∑ –¥–≤—É—Ö –º–µ—Ç–æ–¥–æ–≤
            return max(similarity_shingles, similarity_full)
        
        return similarity_shingles
    
    def _shingle_similarity(self, text1: str, text2: str, n: int = 3) -> float:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ö–æ–¥—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤ –º–µ—Ç–æ–¥–æ–º —à–∏–Ω–≥–ª–æ–≤ (n-–≥—Ä–∞–º–º —Å–ª–æ–≤)
        
        Args:
            text1, text2: –¢–µ–∫—Å—Ç—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            n: –†–∞–∑–º–µ—Ä —à–∏–Ω–≥–ª–∞ (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤)
        
        Returns:
            float –æ—Ç 0.0 –¥–æ 1.0
        """
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
        words1 = text1.split()
        words2 = text2.split()
        
        if len(words1) < n or len(words2) < n:
            # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–∏–º–≤–æ–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
            return SequenceMatcher(None, text1, text2).ratio()
        
        # –°–æ–∑–¥–∞–µ–º —à–∏–Ω–≥–ª—ã
        shingles1 = set(
            ' '.join(words1[i:i+n]) 
            for i in range(len(words1) - n + 1)
        )
        shingles2 = set(
            ' '.join(words2[i:i+n]) 
            for i in range(len(words2) - n + 1)
        )
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ñ–∞–∫–∫–∞—Ä–∞
        if not shingles1 or not shingles2:
            return 0.0
        
        intersection = len(shingles1 & shingles2)
        union = len(shingles1 | shingles2)
        
        jaccard = intersection / union if union > 0 else 0.0
        
        return jaccard
    
    def _check_web_duplicates(self, text: str) -> float:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        –¢—Ä–µ–±—É–µ—Ç API –∞–Ω—Ç–∏–ø–ª–∞–≥–∏–∞—Ç–∞
        
        Returns:
            float —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –æ—Ç 0.0 –¥–æ 100.0
        """
        # TODO: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –±–µ—Å–ø–ª–∞—Ç–Ω—ã–º–∏ API –∞–Ω—Ç–∏–ø–ª–∞–≥–∏–∞—Ç–∞
        # –í–æ–∑–º–æ–∂–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã:
        # - text.ru API (–±–µ—Å–ø–ª–∞—Ç–Ω—ã–π –ª–∏–º–∏—Ç)
        # - content-watch.ru API
        # - advego.com API
        
        logger.info(f"   ‚ÑπÔ∏è  –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –ø–æ–∫–∞ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞")
        return 100.0


def check_text_uniqueness(post) -> Dict:
    """
    –£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
    
    Args:
        post: –û–±—ä–µ–∫—Ç —Å—Ç–∞—Ç—å–∏
    
    Returns:
        Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø—Ä–æ–≤–µ—Ä–∫–∏
    """
    checker = TextUniquenessChecker()
    is_unique, uniqueness, message = checker.check_uniqueness(post)
    
    return {
        'is_unique': is_unique,
        'uniqueness_percent': uniqueness,
        'message': message,
        'min_required': checker.MIN_UNIQUENESS
    }

