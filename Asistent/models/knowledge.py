"""
–ú–æ–¥–µ–ª–∏: AIKnowledgeBase
"""
from django.db import models
from django.contrib.auth.models import User
from django.utils import timezone
from django.core.validators import MinValueValidator, MaxValueValidator
from decimal import Decimal

class AIKnowledgeBase(models.Model):
    
    CATEGORY_CHOICES = [
        ('–ø—Ä–æ–º–ø—Ç—ã', '–ü—Ä–æ–º–ø—Ç—ã'),
        ('–ø—Ä–∞–≤–∏–ª–∞', '–ü—Ä–∞–≤–∏–ª–∞'),
        ('–ø—Ä–∏–º–µ—Ä—ã', '–ü—Ä–∏–º–µ—Ä—ã'),
        ('–∫–æ–º–∞–Ω–¥—ã', '–ö–æ–º–∞–Ω–¥—ã'),
        ('faq', '–ß–∞—Å—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã'),
        ('–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏', '–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏'),
        ('–∏—Å—Ç–æ—á–Ω–∏–∫–∏', '–ò—Å—Ç–æ—á–Ω–∏–∫–∏'),  # –ü—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
    ]
    
    category = models.CharField(
        max_length=100,
        choices=CATEGORY_CHOICES,
        verbose_name='–ö–∞—Ç–µ–≥–æ—Ä–∏—è'
    )
    
    title = models.CharField(
        max_length=300,
        verbose_name='–ó–∞–≥–æ–ª–æ–≤–æ–∫'
    )
    
    content = models.TextField(
        verbose_name='–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ'
    )
    
    tags = models.JSONField(
        default=list,
        verbose_name='–¢–µ–≥–∏',
        help_text='–°–ø–∏—Å–æ–∫ —Ç–µ–≥–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞'
    )
    
    embedding = models.JSONField(
        null=True,
        blank=True,
        verbose_name='–í–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ',
        help_text='–î–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)'
    )
    
    is_active = models.BooleanField(
        default=True,
        verbose_name='–ê–∫—Ç–∏–≤–µ–Ω'
    )
    
    usage_count = models.IntegerField(
        default=0,
        verbose_name='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–π'
    )
    
    priority = models.IntegerField(
        default=50,
        validators=[MinValueValidator(0), MaxValueValidator(100)],
        verbose_name='–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç',
        help_text='0-100, —á–µ–º –≤—ã—à–µ - —Ç–µ–º –≤–∞–∂–Ω–µ–µ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–µ—Ä–≤—ã–º)'
    )
    
    created_by = models.ForeignKey(
        User,
        on_delete=models.CASCADE,
        related_name='knowledge_entries',
        verbose_name='–°–æ–∑–¥–∞–ª'
    )
    
    created_at = models.DateTimeField(
        auto_now_add=True,
        verbose_name='–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è'
    )
    
    updated_at = models.DateTimeField(
        auto_now=True,
        verbose_name='–î–∞—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è'
    )
    
    class Meta:
        verbose_name = 'ü§ñ AI-–ê–≥–µ–Ω—Ç: –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π'
        verbose_name_plural = 'ü§ñ AI-–ê–≥–µ–Ω—Ç: –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π'
        ordering = ['-priority', '-usage_count', '-created_at']  # –°–Ω–∞—á–∞–ª–∞ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        indexes = [
            models.Index(fields=['category', '-priority'], name='kb_cat_prior_idx'),
            models.Index(fields=['-usage_count'], name='kb_usage_idx'),
            models.Index(fields=['is_active', 'category'], name='kb_active_cat_idx'),
            models.Index(fields=['-created_at'], name='kb_created_idx'),
        ]
    
    def __str__(self):
        return f"{self.get_category_display()}: {self.title}"
    
    def increment_usage(self):
        """–£–≤–µ–ª–∏—á–∏—Ç—å —Å—á–µ—Ç—á–∏–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–π"""
        self.usage_count += 1
        self.save(update_fields=['usage_count'])
    
    @staticmethod
    def find_similar(query_text, top_k=5, category=None, min_similarity=0.0):
        """
        –ù–∞—Ö–æ–¥–∏—Ç —Ç–æ–ø-K –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–ø–∏—Å–µ–π –ø–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É
        
        Args:
            query_text: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 5)
            category: –§–∏–ª—å—Ç—Ä –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            min_similarity: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞ (0.0-1.0)
            
        Returns:
            List[Tuple[AIKnowledgeBase, float]]: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (–∑–∞–ø–∏—Å—å, —Å—Ö–æ–∂–µ—Å—Ç—å)
            
        Example:
            >>> results = AIKnowledgeBase.find_similar("–ö–∞–∫ —Å—Ç–∞—Ç—å –∞–≤—Ç–æ—Ä–æ–º?", top_k=3)
            >>> for item, similarity in results:
            ...     print(f"{item.title}: {similarity:.2%}")
        """
        import logging
        logger = logging.getLogger(__name__)
        
        try:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embedding –∑–∞–ø—Ä–æ—Å–∞
            from .gigachat_api import get_embeddings
            import numpy as np
            
            query_embedding = np.array(get_embeddings(query_text))
            
            if len(query_embedding) == 0:
                logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å embedding –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫")
                # Fallback –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫
                return AIKnowledgeBase._fallback_text_search(query_text, top_k, category)
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∞–∫—Ç–∏–≤–Ω—ã–µ –∑–∞–ø–∏—Å–∏ —Å embeddings
            items = AIKnowledgeBase.objects.filter(
                is_active=True,
                embedding__isnull=False
            ).exclude(embedding=[])
            
            if category:
                items = items.filter(category=category)
            
            similarities = []
            
            for item in items:
                try:
                    item_embedding = np.array(item.embedding)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤
                    if item_embedding.shape != query_embedding.shape:
                        continue
                    
                    # –ö–æ—Å–∏–Ω—É—Å–Ω–∞—è –±–ª–∏–∑–æ—Å—Ç—å = dot(A, B) / (norm(A) * norm(B))
                    dot_product = np.dot(query_embedding, item_embedding)
                    norm_query = np.linalg.norm(query_embedding)
                    norm_item = np.linalg.norm(item_embedding)
                    
                    if norm_query == 0 or norm_item == 0:
                        continue
                    
                    similarity = dot_product / (norm_query * norm_item)
                    
                    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º—É –ø–æ—Ä–æ–≥—É
                    if similarity >= min_similarity:
                        similarities.append((item, float(similarity)))
                        
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á—ë—Ç–∞ similarity –¥–ª—è {item.id}: {e}")
                    continue
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–∂–µ—Å—Ç–∏
            similarities.sort(key=lambda x: x[1], reverse=True)
            
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á—ë—Ç—á–∏–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–π –¥–ª—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
            for item, _ in similarities[:top_k]:
                item.increment_usage()
            
            logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(similarities[:top_k])} –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–ø–∏—Å–µ–π")
            return similarities[:top_k]
            
        except ImportError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ numpy: {e}. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install numpy")
            return AIKnowledgeBase._fallback_text_search(query_text, top_k, category)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            return AIKnowledgeBase._fallback_text_search(query_text, top_k, category)
    
    @staticmethod
    def _fallback_text_search(query_text, top_k=5, category=None):
        """
        –†–µ–∑–µ—Ä–≤–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –ø—Ä–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ
        
        Args:
            query_text: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            category: –§–∏–ª—å—Ç—Ä –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            
        Returns:
            List[Tuple[AIKnowledgeBase, float]]: –°–ø–∏—Å–æ–∫ —Å —Ñ–∏–∫—Ç–∏–≤–Ω—ã–º similarity=0.5
        """
        import logging
        logger = logging.getLogger(__name__)
        
        logger.info("üîç –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π fallback –ø–æ–∏—Å–∫")
        
        words = query_text.lower().split()
        items = AIKnowledgeBase.objects.filter(is_active=True)
        
        if category:
            items = items.filter(category=category)
        
        results = []
        for item in items:
            # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Å—á—ë—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π —Å–ª–æ–≤ –≤ title, content –ò —Ç–µ–≥–∞—Ö
            text = f"{item.title} {item.content}".lower()
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–≥–∏ –∫ —Ç–µ–∫—Å—Ç—É –ø–æ–∏—Å–∫–∞
            if hasattr(item, 'tags') and item.tags:
                tags_text = " ".join(str(tag) for tag in item.tags)
                text += " " + tags_text.lower()
            
            matches = sum(1 for word in words if word in text)
            
            if matches > 0:
                # –§–∏–∫—Ç–∏–≤–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
                similarity = min(matches / len(words), 1.0)
                results.append((item, similarity))
        
        results.sort(key=lambda x: x[1], reverse=True)
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á—ë—Ç—á–∏–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–π
        for item, _ in results[:top_k]:
            item.increment_usage()
        
        return results[:top_k]



